> Task :protobuf-models:protobufDummy UP-TO-DATE
> Task :protobuf-models:extractIncludeProto UP-TO-DATE
> Task :protobuf-models:extractProto UP-TO-DATE
> Task :protobuf-models:generateProto UP-TO-DATE
> Task :protobuf-models:compileJava UP-TO-DATE
> Task :util:compileJava NO-SOURCE
> Task :pipeline-service-core:compileJava UP-TO-DATE
> Task :pipeline-service-core:processResources UP-TO-DATE
> Task :pipeline-service-core:classes UP-TO-DATE
> Task :pipeline-service-core:jar UP-TO-DATE
> Task :protobuf-models:processResources UP-TO-DATE
> Task :protobuf-models:classes UP-TO-DATE
> Task :protobuf-models:jar UP-TO-DATE
> Task :util:processResources UP-TO-DATE
> Task :util:classes UP-TO-DATE
> Task :util:jar UP-TO-DATE
> Task :pipeline-service-test-utils:pipeline-test-platform:compileJava
> Task :pipeline-service-test-utils:pipeline-test-platform:processResources NO-SOURCE
> Task :pipeline-service-test-utils:pipeline-test-platform:classes
> Task :pipeline-service-test-utils:pipeline-test-platform:compileTestJava
> Task :pipeline-service-test-utils:pipeline-test-platform:compileTestResourcesJava NO-SOURCE
> Task :pipeline-service-test-utils:pipeline-test-platform:inspectRuntimeClasspath UP-TO-DATE
> Task :pipeline-service-test-utils:pipeline-test-platform:processTestResourcesResources NO-SOURCE
> Task :pipeline-service-test-utils:pipeline-test-platform:testResourcesClasses UP-TO-DATE
[34m[test-resources-service][0;39m [36m11:54:37.136[0;39m [1;30m[main][0;39m [34mINFO [0;39m [35mi.m.c.DefaultApplicationContext$RuntimeConfiguredEnvironment[0;39m - Established active environments: [test]
[34m[test-resources-service][0;39m [36m11:54:37.138[0;39m [1;30m[ForkJoinPool.commonPool-worker-1][0;39m [34mINFO [0;39m [35mi.m.t.e.TestResourcesResolverLoader[0;39m - Loaded 3 test resources resolvers: io.micronaut.testresources.kafka.KafkaTestResourceProvider, io.micronaut.testresources.hashicorp.vault.VaultTestResourceProvider, io.micronaut.testresources.testcontainers.GenericTestContainerProvider
[34m[test-resources-service][0;39m [36m11:54:37.158[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.DockerClientFactory[0;39m - Testcontainers version: 2.8.0
[34m[test-resources-service][0;39m [36m11:54:37.254[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.t.d.DockerClientProviderStrategy[0;39m - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
[34m[test-resources-service][0;39m [36m11:54:37.315[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.t.d.DockerClientProviderStrategy[0;39m - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
[34m[test-resources-service][0;39m [36m11:54:37.316[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.DockerClientFactory[0;39m - Docker host IP address is localhost
[34m[test-resources-service][0;39m [36m11:54:37.327[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.DockerClientFactory[0;39m - Connected to docker: 
  Server Version: 28.0.4
  API Version: 1.48
  Operating System: Docker Desktop
  Total Memory: 14967 MB
  Labels: 
    com.docker.desktop.address=unix:///Users/krickert/Library/Containers/com.docker.docker/Data/docker-cli.sock
[34m[test-resources-service][0;39m [36m11:54:37.332[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.images.PullPolicy[0;39m - Image pull policy will be performed by: DefaultPullPolicy()
[34m[test-resources-service][0;39m [36m11:54:37.332[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.t.utility.ImageNameSubstitutor[0;39m - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
[34m[test-resources-service][0;39m [36m11:54:37.566[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mtc.testcontainers/ryuk:0.11.0[0;39m - Creating container for image: testcontainers/ryuk:0.11.0
[34m[test-resources-service][0;39m [36m11:54:37.786[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mtc.testcontainers/ryuk:0.11.0[0;39m - Container testcontainers/ryuk:0.11.0 is starting: d369c06846949e9e95406409a658f093489f26a57935e1cd46b9b767ebe1eab0
[34m[test-resources-service][0;39m [36m11:54:38.020[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mtc.testcontainers/ryuk:0.11.0[0;39m - Container testcontainers/ryuk:0.11.0 started in PT0.454337S
[34m[test-resources-service][0;39m [36m11:54:38.025[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.t.utility.RyukResourceReaper[0;39m - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
[34m[test-resources-service][0;39m [36m11:54:38.025[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.DockerClientFactory[0;39m - Checking the system...
[34m[test-resources-service][0;39m [36m11:54:38.025[0;39m [1;30m[pool-1-thread-1][0;39m [34mINFO [0;39m [35mo.testcontainers.DockerClientFactory[0;39m - âœ”ï¸Ž Docker server version should be at least 1.6.0
[34m[test-resources-service][0;39m [36m11:54:38.094[0;39m [1;30m[scheduled-executor-thread-1][0;39m [34mINFO [0;39m [35mi.m.t.server.ExpiryManager[0;39m - Test resources server will automatically be shutdown if it doesn't receive requests for 60 minutes
[34m[test-resources-service][0;39m [36m11:54:38.110[0;39m [1;30m[main][0;39m [34mINFO [0;39m [35mi.m.t.server.TestResourcesService[0;39m - A Micronaut Test Resources server is listening on port 57987, started in 1025ms
> Task :pipeline-service-test-utils:pipeline-test-platform:internalStartTestResourcesService
Test resources server started in standalone mode. You can stop it by running the stopTestResourcesService task.
> Task :pipeline-service-test-utils:pipeline-test-platform:processTestResources UP-TO-DATE
> Task :pipeline-service-test-utils:pipeline-test-platform:testClasses
11:54:38,588 |-INFO in ch.qos.logback.classic.LoggerContext[default] - This is logback-classic version 1.5.16
11:54:38,589 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - No custom configurators were discovered as a service.
11:54:38,589 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - Trying to configure with ch.qos.logback.classic.joran.SerializedModelConfigurator
11:54:38,589 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - Constructed configurator of type class ch.qos.logback.classic.joran.SerializedModelConfigurator
11:54:38,590 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.scmo]
11:54:38,590 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.scmo]
11:54:38,592 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - ch.qos.logback.classic.joran.SerializedModelConfigurator.configure() call lasted 1 milliseconds. ExecutionStatus=INVOKE_NEXT_IF_ANY
11:54:38,592 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - Trying to configure with ch.qos.logback.classic.util.DefaultJoranConfigurator
11:54:38,592 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - Constructed configurator of type class ch.qos.logback.classic.util.DefaultJoranConfigurator
11:54:38,593 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]
11:54:38,593 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/protobuf-models/build/libs/protobuf-models-1.0.0-SNAPSHOT.jar!/logback.xml]
11:54:38,594 |-WARN in ch.qos.logback.classic.util.DefaultJoranConfigurator@73c60324 - Resource [logback.xml] occurs multiple times on the classpath.
11:54:38,594 |-WARN in ch.qos.logback.classic.util.DefaultJoranConfigurator@73c60324 - Resource [logback.xml] occurs at [jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/pipeline-service-core/build/libs/pipeline-service-core-1.0.0-SNAPSHOT.jar!/logback.xml]
11:54:38,594 |-WARN in ch.qos.logback.classic.util.DefaultJoranConfigurator@73c60324 - Resource [logback.xml] occurs at [jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/util/build/libs/util-1.0.0-SNAPSHOT.jar!/logback.xml]
11:54:38,594 |-WARN in ch.qos.logback.classic.util.DefaultJoranConfigurator@73c60324 - Resource [logback.xml] occurs at [jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/protobuf-models/build/libs/protobuf-models-1.0.0-SNAPSHOT.jar!/logback.xml]
11:54:38,596 |-INFO in ConfigurationWatchList(mainURL=jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/protobuf-models/build/libs/protobuf-models-1.0.0-SNAPSHOT.jar!/logback.xml, fileWatchList={}, urlWatchList=[}) - URL [jar:file:/Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/protobuf-models/build/libs/protobuf-models-1.0.0-SNAPSHOT.jar!/logback.xml] is not of type file
11:54:38,630 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - Processing appender named [STDOUT]
11:54:38,630 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]
11:54:38,633 |-INFO in ch.qos.logback.core.model.processor.ImplicitModelHandler - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property
11:54:38,644 |-INFO in ch.qos.logback.core.ConsoleAppender[STDOUT] - BEWARE: Writing to the console can be very slow. Avoid logging to the 
11:54:38,644 |-INFO in ch.qos.logback.core.ConsoleAppender[STDOUT] - console in production environments, especially in high volume systems.
11:54:38,644 |-INFO in ch.qos.logback.core.ConsoleAppender[STDOUT] - See also https://logback.qos.ch/codes.html#slowConsole
11:54:38,644 |-INFO in ch.qos.logback.classic.model.processor.RootLoggerModelHandler - Setting level of ROOT logger to INFO
11:54:38,644 |-INFO in ch.qos.logback.core.model.processor.AppenderRefModelHandler - Attaching appender named [STDOUT] to Logger[ROOT]
11:54:38,645 |-INFO in ch.qos.logback.classic.model.processor.LoggerModelHandler - Setting level of logger [com.krickert.search] to INFO
11:54:38,645 |-INFO in ch.qos.logback.classic.model.processor.LoggerModelHandler - Setting level of logger [io.micronaut] to INFO
11:54:38,645 |-INFO in ch.qos.logback.classic.model.processor.LoggerModelHandler - Setting level of logger [io.netty] to INFO
11:54:38,645 |-INFO in ch.qos.logback.core.model.processor.DefaultProcessor@7e3f95fe - End of configuration.
11:54:38,645 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@34625ccd - Registering current configuration as safe fallback point
11:54:38,645 |-INFO in ch.qos.logback.classic.util.ContextInitializer@58bf8650 - ch.qos.logback.classic.util.DefaultJoranConfigurator.configure() call lasted 53 milliseconds. ExecutionStatus=DO_NOT_INVOKE_NEXT_IF_ANY

11:54:38.700 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Creating TestContainerManager instance...
11:54:38.700 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Initializing TestContainerManager with registry type: none
11:54:38.704 [Test worker] INFO  o.testcontainers.DockerClientFactory - Testcontainers version: 1.20.6
11:54:38.795 [Test worker] INFO  o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
11:54:38.911 [Test worker] INFO  o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
11:54:38.912 [Test worker] INFO  o.testcontainers.DockerClientFactory - Docker host IP address is localhost
11:54:38.928 [Test worker] INFO  o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 28.0.4
  API Version: 1.48
  Operating System: Docker Desktop
  Total Memory: 14967 MB
  Labels: 
    com.docker.desktop.address=unix:///Users/krickert/Library/Containers/com.docker.docker/Data/docker-cli.sock
11:54:38.935 [Test worker] INFO  o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
11:54:38.936 [Test worker] INFO  o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
11:54:39.150 [Test worker] INFO  tc.testcontainers/ryuk:0.11.0 - Creating container for image: testcontainers/ryuk:0.11.0
11:54:39.295 [Test worker] INFO  tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 is starting: eb27d208aeae01a5b3164278cbded8d5e1d58ad376c992e29cf453d47edbc5aa
11:54:39.499 [Test worker] INFO  tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 started in PT0.349274S
11:54:39.502 [Test worker] INFO  o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
11:54:39.502 [Test worker] INFO  o.testcontainers.DockerClientFactory - Checking the system...
11:54:39.502 [Test worker] INFO  o.testcontainers.DockerClientFactory - âœ”ï¸Ž Docker server version should be at least 1.6.0
11:54:39.521 [Test worker] INFO  c.k.s.t.p.consul.ConsulContainer - Defining Consul Testcontainer (extending GenericContainer)...
11:54:39.522 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Consul container configured.
11:54:39.523 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Kafka container configured.
11:54:39.523 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - No schema registry container configured (type: none).
11:54:39.523 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Starting managed containers...
11:54:39.523 [Test worker] INFO  c.k.s.t.p.consul.ConsulContainer - Attempting to start Consul container (ConsulContainer.start())...
11:54:39.523 [Test worker] INFO  tc.hashicorp/consul:latest - Creating container for image: hashicorp/consul:latest
11:54:39.558 [Test worker] INFO  tc.hashicorp/consul:latest - Container hashicorp/consul:latest is starting: 7a6c438456b7fcb84d84794aecb2f1a59c0331b56683e0fcc3bf902d99caa975
11:54:39.767 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: ==> Starting Consul agent...
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:                Version: '1.20.5'
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:             Build Date: '2025-03-11 10:16:18 +0000 UTC'
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:                Node ID: 'c4c11f47-254e-52a1-d3e8-d56835e3873a'
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:              Node name: '7a6c438456b7'
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:             Datacenter: 'dc1' (Segment: '<all>')
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:                 Server: true (Bootstrap: false)
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:            Client Addr: [0.0.0.0] (HTTP: 8500, HTTPS: -1, gRPC: 8502, gRPC-TLS: 8503, DNS: 8600)
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:           Cluster Addr: 127.0.0.1 (LAN: 8301, WAN: 8302)
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:      Gossip Encryption: false
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:       Auto-Encrypt-TLS: false
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:            ACL Enabled: false
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:      Reporting Enabled: false
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:     ACL Default Policy: allow
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:              HTTPS TLS: Verify Incoming: false, Verify Outgoing: false, Min Version: TLSv1_2
11:54:39.768 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:               gRPC TLS: Verify Incoming: false, Min Version: TLSv1_2
11:54:39.769 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT:       Internal RPC TLS: Verify Incoming: false, Verify Outgoing: false (Verify Hostname: false), Min Version: TLSv1_2
11:54:39.769 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 
11:54:39.769 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: ==> Log data will now stream in as it occurs:
11:54:39.769 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.770Z [INFO]  agent.server.raft: initial configuration: index=1 servers="[{Suffrage:Voter ID:c4c11f47-254e-52a1-d3e8-d56835e3873a Address:127.0.0.1:8300}]"
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server.serf.wan: serf: EventMemberJoin: 7a6c438456b7.dc1 127.0.0.1
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server.raft: entering follower state: follower="Node at 127.0.0.1:8300 [Follower]" leader-address= leader-id=
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server.serf.lan: serf: EventMemberJoin: 7a6c438456b7 127.0.0.1
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.router: Initializing LAN area manager
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server: Handled event for server in area: event=member-join server=7a6c438456b7.dc1 area=wan
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server.autopilot: reconciliation now disabled
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.771Z [INFO]  agent.server: Adding LAN server: server="7a6c438456b7 (Addr: tcp/127.0.0.1:8300) (DC: dc1)"
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.772Z [INFO]  agent.server.cert-manager: initialized server certificate management
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.772Z [INFO]  agent: Started DNS server: address=0.0.0.0:8600 network=udp
11:54:39.774 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.772Z [INFO]  agent: Started DNS server: address=0.0.0.0:8600 network=tcp
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/hcp/v2/link
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/internal/v1/tombstone
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v2/album
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v2/festival
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/multicluster/v2/exportedservices
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/multicluster/v2/computedexportedservices
11:54:39.775 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v1/executive
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v1/album
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v2/artist
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v1/artist
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v1/concept
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/multicluster/v2/partitionexportedservices
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/demo/v1/recordlabel
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/multicluster/v2/namespaceexportedservices
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent.http: Registered resource endpoint: endpoint=/api/hcp/v2/telemetrystate
11:54:39.776 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.773Z [INFO]  agent: Starting server: address=[::]:8500 network=tcp protocol=http
11:54:39.777 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.774Z [INFO]  agent: Started gRPC listeners: port_name=grpc_tls address=[::]:8503 network=tcp
11:54:39.778 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.774Z [INFO]  agent: Started gRPC listeners: port_name=grpc address=[::]:8502 network=tcp
11:54:39.778 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.775Z [INFO]  agent: started state syncer
11:54:39.778 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.775Z [INFO]  agent: Consul agent running!
11:54:39.817 [Test worker] INFO  o.t.c.wait.strategy.HttpWaitStrategy - /heuristic_chatterjee: Waiting for 120 seconds for URL: http://localhost:57993/v1/status/leader (where port 57993 maps to container port 8500)
11:54:39.824 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [WARN]  agent.server.raft: heartbeat timeout reached, starting election: last-leader-addr= last-leader-id=
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server.raft: entering candidate state: node="Node at 127.0.0.1:8300 [Candidate]" term=2
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server.raft: pre-vote successful, starting election: term=2 tally=1 refused=0 votesNeeded=1
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server.raft: election won: term=2 tally=1
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server.raft: entering leader state: leader="Node at 127.0.0.1:8300 [Leader]"
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server: cluster leadership acquired
11:54:39.825 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server: New leader elected: payload=7a6c438456b7
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.server.autopilot: reconciliation now enabled
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.leader: started routine: routine="federation state anti-entropy"
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.leader: started routine: routine="federation state pruning"
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.leader: started routine: routine="streaming peering resources"
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.leader: started routine: routine="metrics for streaming peering resources"
11:54:39.826 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.823Z [INFO]  agent.leader: started routine: routine="peering deferred deletion"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  connect.ca: updated root certificates from primary datacenter
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  connect.ca: initialized primary datacenter CA with provider: provider=consul
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="intermediate cert renew watch"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="CA root pruning"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="CA root expiration metric"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="CA signing expiration metric"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="virtual IP version check"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.leader: started routine: routine="config entry controllers"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.824Z [INFO]  agent.server: member joined, marking health alive: member=7a6c438456b7 partition=default
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.825Z [INFO]  agent.leader: stopping routine: routine="virtual IP version check"
11:54:39.828 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.825Z [INFO]  agent.leader: stopped routine: routine="virtual IP version check"
11:54:39.831 [Test worker] INFO  tc.hashicorp/consul:latest - Container hashicorp/consul:latest started in PT0.307864S
11:54:39.832 [Test worker] INFO  c.k.s.t.p.consul.ConsulContainer - Consul container started successfully. Endpoint: http://localhost:57993
11:54:39.832 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Consul container started.
11:54:39.832 [Test worker] INFO  tc.apache/kafka:latest - Creating container for image: apache/kafka:latest
11:54:39.872 [Test worker] INFO  tc.apache/kafka:latest - Container apache/kafka:latest is starting: 16c85b9e526bf050d0bf2ebf96eeab30e428973294a0cd9a40a61f24abc3aedf
11:54:39.988 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:39.986Z [INFO]  agent: Synced node info
11:54:40.093 [Test worker] WARN  tc.apache/kafka:latest - The architecture '' for image 'apache/kafka:latest' (ID sha256:3f7b939115cd4872e9cee9369d80bd69712fde55f9902f46d793f64848dedc75) does not match the Docker server architecture 'arm64'. This will cause the container to execute much more slowly due to emulation and may lead to timeout failures.
11:54:40.262 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:40.261Z [INFO]  agent.server: federation state anti-entropy synced
11:54:42.323 [Test worker] INFO  tc.apache/kafka:latest - Container apache/kafka:latest started in PT2.491255S
11:54:42.323 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Kafka container started.
11:54:42.323 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - All managed containers started.
11:54:42.335 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - No schema registry endpoint configured (none)
11:54:42.335 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - Properties populated: [kafka.brokers, consul.client.defaultZone, kafka.schema.registry.type, kafka.bootstrap.servers, kafka.producers.default.key.serializer, kafka.consumers.default.key.deserializer, kafka.consumers.default.value.deserializer, consul.client.registration.enabled, kafka.producers.default.value.serializer, consul.host, consul.port]
11:54:42.336 [Test worker] INFO  c.k.s.t.p.kafka.TestContainerManager - TestContainerManager initialized successfully.
11:54:42.344 [Test worker] INFO  c.k.s.t.p.AbstractPipelineTest - AbstractPipelineTest getProperties() fetching base properties from TestContainerManager
11:54:42.344 [Test worker] INFO  c.k.s.t.p.TestPipelineServiceProcessorTest - Providing Micronaut test properties for TestPipelineServiceProcessorTest: [consul.client.defaultZone, kafka.consumers.default.key.deserializer, pipeline.service.name, kafka.producers.default.value.serializer, kafka.brokers, kafka.schema.registry.type, kafka.bootstrap.servers, kafka.producers.default.key.serializer, kafka.consumers.default.value.deserializer, consul.client.registration.enabled, kafka.producer.dynamic.enabled, kafka.consumer.dynamic.enabled, consul.host, consul.port]
11:54:42.580 [Test worker] INFO  i.m.c.DefaultApplicationContext$RuntimeConfiguredEnvironment - Established active environments: [test, apicurio, consul]
11:54:42.581 [Test worker] INFO  i.m.c.DefaultApplicationContext$BootstrapEnvironment - Established active environments: [test, apicurio, consul]
11:54:42.664 [Test worker] INFO  i.m.s.ObjectMappers$ObjectMapperContext$1 - Established active environments: [test]
11:54:42.761 [Test worker] INFO  i.m.context.DefaultBeanContext - Reading bootstrap environment configuration
11:54:42.854 [Test worker] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:57995]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

11:54:42.921 [Test worker] INFO  o.a.k.c.admin.AdminClientConfig - These configurations '[producer.dynamic.enabled, config.update.listener.group-id, registry.type, brokers, config.update.listener.topic, config.update.listener.enabled, consumer.dynamic.enabled, enabled, schema.registry.type, health.enabled]' were supplied but are not used yet.
11:54:42.921 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.9.0
11:54:42.921 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 84caaa6e9da06435
11:54:42.921 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1745942082921
11:54:42.944 [Test worker] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:57995]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = pipeline-test-platform-pipe-stream-output-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pipeline-test-output-consumer-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

11:54:42.972 [Test worker] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
11:54:43.001 [Test worker] INFO  o.a.k.c.consumer.ConsumerConfig - These configurations '[brokers, config.update.listener.topic, config.update.listener.enabled, consumer.dynamic.enabled, enabled, schema.registry.type, producer.dynamic.enabled, config.update.listener.group-id, registry.type, health.enabled]' were supplied but are not used yet.
11:54:43.001 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.9.0
11:54:43.001 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 84caaa6e9da06435
11:54:43.001 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1745942083001
11:54:43.002 [Test worker] INFO  c.k.s.t.p.AbstractPipelineTest$PipeStreamOutputConsumer - PipeStreamOutputConsumer initialized, listening on fixed topic: pipeline-test-output-topic
11:54:43.003 [Test worker] INFO  o.a.k.c.c.i.ClassicKafkaConsumer - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Subscribed to topic(s): pipeline-test-output-topic
11:54:43.003 [Test worker] INFO  i.m.c.k.p.KafkaConsumerProcessor - Kafka listener [PipeStreamOutputConsumer#receive] subscribed to topics: [pipeline-test-output-topic]
11:54:43.104 [pool-1-thread-1] INFO  i.m.c.k.p.KafkaConsumerProcessor - Consumer [pipeline-test-platform-pipe-stream-output-consumer] assignments changed: null -> []
11:54:43.106 [Test worker] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:57995]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = pipeline-test-platform-config-update-kafka-listener
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = config-update-listeners-pipeline-test-platform
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

11:54:43.107 [Test worker] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
11:54:43.112 [Test worker] INFO  o.a.k.c.consumer.ConsumerConfig - These configurations '[brokers, config.update.listener.topic, config.update.listener.enabled, consumer.dynamic.enabled, enabled, schema.registry.type, producer.dynamic.enabled, config.update.listener.group-id, registry.type, health.enabled]' were supplied but are not used yet.
11:54:43.112 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.9.0
11:54:43.112 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 84caaa6e9da06435
11:54:43.112 [Test worker] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1745942083112
11:54:43.113 [Test worker] INFO  o.a.k.c.c.i.ClassicKafkaConsumer - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Subscribed to topic(s): config-updates
11:54:43.113 [Test worker] INFO  i.m.c.k.p.KafkaConsumerProcessor - Kafka listener [ConfigUpdateKafkaListener#receiveConfigUpdate] subscribed to topics: [config-updates]
11:54:43.115 [pool-1-thread-2] INFO  i.m.c.k.p.KafkaConsumerProcessor - Consumer [pipeline-test-platform-config-update-kafka-listener] assignments changed: null -> []
11:54:43.144 [pool-1-thread-2] WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] The metadata response from the cluster reported a recoverable issue with correlation id 2 : {config-updates=UNKNOWN_TOPIC_OR_PARTITION}
11:54:43.144 [pool-1-thread-1] WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] The metadata response from the cluster reported a recoverable issue with correlation id 2 : {pipeline-test-output-topic=UNKNOWN_TOPIC_OR_PARTITION}
11:54:43.145 [pool-1-thread-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Cluster ID: 4L6g3nShT-eMCtK--X86sw
11:54:43.145 [pool-1-thread-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Cluster ID: 4L6g3nShT-eMCtK--X86sw
11:54:43.296 [Test worker] INFO  c.k.s.pipeline.grpc.GrpcForwarder - Initializing GrpcForwarder with plaintext: true
11:54:43.322 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Discovered group coordinator localhost:57995 (id: 2147483646 rack: null)
11:54:43.322 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Discovered group coordinator localhost:57995 (id: 2147483646 rack: null)
11:54:43.325 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] (Re-)joining group
11:54:43.326 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] (Re-)joining group
11:54:43.330 [Test worker] INFO  c.k.s.p.grpc.PipelineServiceImpl - Using PipelineServiceProcessor implementation: com.krickert.search.test.platform.TestPipelineServiceProcessor
11:54:43.337 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Request joining group due to: need to re-join with the given member-id: pipeline-test-platform-config-update-kafka-listener-fd3f2e08-1091-4466-ba8d-050b350a875a
11:54:43.338 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] (Re-)joining group
11:54:43.342 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Request joining group due to: need to re-join with the given member-id: pipeline-test-platform-pipe-stream-output-consumer-5025f5ac-4f9d-47fa-a16b-8efc9d3aba55
11:54:43.342 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] (Re-)joining group
11:54:43.348 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Successfully joined group with generation Generation{generationId=1, memberId='pipeline-test-platform-config-update-kafka-listener-fd3f2e08-1091-4466-ba8d-050b350a875a', protocol='range'}
11:54:43.353 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Finished assignment for group at generation 1: {pipeline-test-platform-config-update-kafka-listener-fd3f2e08-1091-4466-ba8d-050b350a875a=Assignment(partitions=[config-updates-0])}
11:54:43.359 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Successfully joined group with generation Generation{generationId=1, memberId='pipeline-test-platform-pipe-stream-output-consumer-5025f5ac-4f9d-47fa-a16b-8efc9d3aba55', protocol='range'}
11:54:43.360 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Finished assignment for group at generation 1: {pipeline-test-platform-pipe-stream-output-consumer-5025f5ac-4f9d-47fa-a16b-8efc9d3aba55=Assignment(partitions=[pipeline-test-output-topic-0])}
11:54:43.366 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Successfully synced group in generation Generation{generationId=1, memberId='pipeline-test-platform-pipe-stream-output-consumer-5025f5ac-4f9d-47fa-a16b-8efc9d3aba55', protocol='range'}
11:54:43.366 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Successfully synced group in generation Generation{generationId=1, memberId='pipeline-test-platform-config-update-kafka-listener-fd3f2e08-1091-4466-ba8d-050b350a875a', protocol='range'}
11:54:43.367 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Notifying assignor about the new Assignment(partitions=[pipeline-test-output-topic-0])
11:54:43.367 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Notifying assignor about the new Assignment(partitions=[config-updates-0])
11:54:43.368 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Adding newly assigned partitions: pipeline-test-output-topic-0
11:54:43.368 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Adding newly assigned partitions: config-updates-0
11:54:43.380 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Found no committed offset for partition config-updates-0
11:54:43.380 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Found no committed offset for partition pipeline-test-output-topic-0
11:54:43.394 [pool-1-thread-1] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Resetting offset for partition pipeline-test-output-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57995 (id: 1 rack: null)], epoch=0}}.
11:54:43.394 [pool-1-thread-2] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Resetting offset for partition config-updates-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:57995 (id: 1 rack: null)], epoch=0}}.
11:54:43.417 [pool-1-thread-1] INFO  i.m.c.k.p.KafkaConsumerProcessor - Consumer [pipeline-test-platform-pipe-stream-output-consumer] assignments changed: [] -> [pipeline-test-output-topic-0]
11:54:43.419 [pool-1-thread-2] INFO  i.m.c.k.p.KafkaConsumerProcessor - Consumer [pipeline-test-platform-config-update-kafka-listener] assignments changed: [] -> [config-updates-0]
11:54:43.445 [Test worker] INFO  i.m.g.s.GrpcEmbeddedServerListener - GRPC started on port 17319
11:54:43.458 [Test worker] INFO  i.m.c.k.p.KafkaConsumerGroupManager - Application shutdown initiated. Preparing to delete registered Kafka unique consumer groups.
11:54:43.458 [Test worker] INFO  i.m.c.k.p.KafkaConsumerGroupManager - No unique consumer groups are registered for deletion.
11:54:43.477 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:43.476Z [ERROR] agent.http: Request error: method=PUT url="/v1/agent/service/deregister/pipeline-test-platform%3A58003" from=172.21.0.1:56212 error="Unknown service ID \"pipeline-test-platform:58003\". Ensure that the service ID is passed, not the service name."
11:54:43.480 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:43.479Z [INFO]  agent: Synced service: service=pipeline-test-platform:17319
11:54:43.480 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:43.479Z [INFO]  agent: Synced service: service=pipeline-test-platform:58003
11:54:43.697 [default-nioEventLoopGroup-1-5] INFO  i.m.d.registration.AutoRegistration - Registered service [pipeline-test-platform] with Consul
11:54:43.697 [default-nioEventLoopGroup-1-14] INFO  i.m.d.registration.AutoRegistration - Registered service [pipeline-test-platform] with Consul
11:54:46.716 [docker-java-stream-1351699218] INFO  c.k.s.t.p.consul.ConsulContainer - [CONSUL] STDOUT: 2025-04-29T15:54:46.715Z [INFO]  agent: Deregistered service: service=pipeline-test-platform:58003
11:54:46.717 [Test worker] INFO  i.m.d.registration.AutoRegistration - De-registered service [pipeline-test-platform] with Consul
11:54:46.740 [Test worker] INFO  i.m.c.k.p.KafkaConsumerGroupManager - Application shutdown initiated. Preparing to delete registered Kafka unique consumer groups.
11:54:46.740 [Test worker] INFO  i.m.c.k.p.KafkaConsumerGroupManager - No unique consumer groups are registered for deletion.
11:54:46.946 [default-nioEventLoopGroup-1-3] ERROR i.m.h.client.netty.DefaultHttpClient - Failed to connect to remote
java.lang.IllegalStateException: executor not accepting a task
	at io.netty.resolver.AddressResolverGroup.getResolver(AddressResolverGroup.java:61)
	at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)
	at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:1583)
11:54:55.861 [Test worker] ERROR i.m.d.registration.AutoRegistration - Error occurred de-registering service [pipeline-test-platform] with Consul: event executor terminated
java.util.concurrent.RejectedExecutionException: event executor terminated
	at io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:934)
	at io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:353)
	at io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:346)
	at io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:836)
	at io.netty.util.concurrent.SingleThreadEventExecutor.lazyExecute0(SingleThreadEventExecutor.java:831)
	at io.netty.util.concurrent.SingleThreadEventExecutor.lazyExecute(SingleThreadEventExecutor.java:822)
	at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:292)
	at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:205)
	at io.netty.util.concurrent.AbstractEventExecutorGroup.schedule(AbstractEventExecutorGroup.java:50)
	at io.netty.util.concurrent.AbstractEventExecutorGroup.schedule(AbstractEventExecutorGroup.java:32)
	at io.micronaut.core.execution.ExecutionFlow.timeout(ExecutionFlow.java:188)
	at io.micronaut.http.client.netty.DefaultHttpClient.exchange(DefaultHttpClient.java:918)
	at io.micronaut.http.client.netty.DefaultHttpClient.exchange(DefaultHttpClient.java:883)
	at io.micronaut.http.client.netty.DefaultHttpClient.retrieve(DefaultHttpClient.java:990)
	at io.micronaut.http.client.interceptor.HttpClientIntroductionAdvice.lambda$httpClientResponsePublisher$13(HttpClientIntroductionAdvice.java:554)
	at reactor.core.publisher.FluxFlatMap$FlatMapMain.onNext(FluxFlatMap.java:388)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118)
	at reactor.core.publisher.MonoCallable$MonoCallableSubscription.request(MonoCallable.java:156)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.request(FluxFilterFuseable.java:191)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)
	at reactor.core.publisher.FluxFlatMap$FlatMapMain.onSubscribe(FluxFlatMap.java:373)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onSubscribe(FluxFilterFuseable.java:87)
	at reactor.core.publisher.MonoCallable.subscribe(MonoCallable.java:48)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.core.publisher.FluxDelaySubscription.accept(FluxDelaySubscription.java:59)
	at reactor.core.publisher.FluxDelaySubscription.accept(FluxDelaySubscription.java:36)
	at reactor.core.publisher.FluxDelaySubscription$DelaySubscriptionOtherSubscriber.onNext(FluxDelaySubscription.java:131)
	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270)
	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285)
	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68)
	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:104)
		at reactor.core.publisher.Flux.blockFirst(Flux.java:2766)
		at io.micronaut.discovery.client.registration.DiscoveryServiceAutoRegistration.performDeregistration(DiscoveryServiceAutoRegistration.java:133)
		at io.micronaut.discovery.consul.registration.ConsulAutoRegistration.deregister(ConsulAutoRegistration.java:147)
		at io.micronaut.discovery.registration.AutoRegistration.onApplicationEvent(AutoRegistration.java:61)
		at io.micronaut.discovery.registration.AutoRegistration.onApplicationEvent(AutoRegistration.java:38)
		at io.micronaut.context.event.ApplicationEventPublisherFactory.notifyEventListeners(ApplicationEventPublisherFactory.java:266)
		at io.micronaut.context.event.ApplicationEventPublisherFactory$2.publishEvent(ApplicationEventPublisherFactory.java:226)
		at io.micronaut.context.DefaultBeanContext.publishEvent(DefaultBeanContext.java:1865)
		at io.micronaut.grpc.server.GrpcEmbeddedServer.stop(GrpcEmbeddedServer.java:210)
		at io.micronaut.grpc.server.GrpcEmbeddedServer.stop(GrpcEmbeddedServer.java:63)
		at io.micronaut.context.DefaultBeanContext.destroyLifeCycleBean(DefaultBeanContext.java:1258)
		at io.micronaut.context.DefaultApplicationContext.destroyLifeCycleBean(DefaultApplicationContext.java:715)
		at io.micronaut.context.DefaultBeanContext.destroyBean(DefaultBeanContext.java:1228)
		at io.micronaut.context.DefaultBeanContext.destroyBean(DefaultBeanContext.java:1191)
		at io.micronaut.context.DefaultBeanContext.stop(DefaultBeanContext.java:455)
		at io.micronaut.context.DefaultApplicationContext.stop(DefaultApplicationContext.java:236)
		at io.micronaut.http.server.netty.NettyHttpServer.stopInternal(NettyHttpServer.java:735)
		at io.micronaut.http.server.netty.NettyHttpServer.stop(NettyHttpServer.java:357)
		at io.micronaut.http.server.netty.NettyHttpServer.stop(NettyHttpServer.java:344)
		at io.micronaut.http.server.netty.NettyHttpServer.stop(NettyHttpServer.java:114)
		at io.micronaut.test.extensions.AbstractMicronautExtension.stopEmbeddedApplication(AbstractMicronautExtension.java:557)
		at io.micronaut.test.extensions.AbstractMicronautExtension.afterClass(AbstractMicronautExtension.java:471)
		at io.micronaut.test.extensions.junit5.MicronautJunit5Extension.afterAll(MicronautJunit5Extension.java:209)
		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllCallbacks$19(ClassBasedTestDescriptor.java:462)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllCallbacks$20(ClassBasedTestDescriptor.java:462)
		at org.junit.platform.commons.util.CollectionUtils.forEachInReverseOrder(CollectionUtils.java:244)
		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeAfterAllCallbacks(ClassBasedTestDescriptor.java:461)
		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:236)
		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:85)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:166)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:166)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)
		at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:160)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
		at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
		at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
		at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94)
		at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63)
		at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
		at java.base/java.lang.reflect.Method.invoke(Method.java:580)
		at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
		at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
		at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
		at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92)
		at jdk.proxy1/jdk.proxy1.$Proxy4.stop(Unknown Source)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63)
		at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
		at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:122)
		at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:72)
		at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
		at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
11:54:55.893 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Revoke previously assigned partitions config-updates-0
11:54:55.893 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Revoke previously assigned partitions pipeline-test-output-topic-0
11:54:55.894 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Member pipeline-test-platform-config-update-kafka-listener-fd3f2e08-1091-4466-ba8d-050b350a875a sending LeaveGroup request to coordinator localhost:57995 (id: 2147483646 rack: null) due to the consumer is being closed
11:54:55.894 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Member pipeline-test-platform-pipe-stream-output-consumer-5025f5ac-4f9d-47fa-a16b-8efc9d3aba55 sending LeaveGroup request to coordinator localhost:57995 (id: 2147483646 rack: null) due to the consumer is being closed
11:54:55.894 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Resetting generation and member id due to: consumer pro-actively leaving the group
11:54:55.894 [pool-1-thread-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-pipe-stream-output-consumer, groupId=pipeline-test-output-consumer-group] Request joining group due to: consumer pro-actively leaving the group
11:54:55.894 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Resetting generation and member id due to: consumer pro-actively leaving the group
11:54:55.894 [pool-1-thread-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=pipeline-test-platform-config-update-kafka-listener, groupId=config-update-listeners-pipeline-test-platform] Request joining group due to: consumer pro-actively leaving the group
11:54:56.131 [pool-1-thread-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
11:54:56.131 [pool-1-thread-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
11:54:56.131 [pool-1-thread-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:54:56.131 [pool-1-thread-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:54:56.131 [pool-1-thread-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
11:54:56.131 [pool-1-thread-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
11:54:56.131 [pool-1-thread-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
11:54:56.131 [pool-1-thread-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
11:54:56.136 [pool-1-thread-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for pipeline-test-platform-config-update-kafka-listener unregistered
11:54:56.136 [pool-1-thread-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for pipeline-test-platform-pipe-stream-output-consumer unregistered
11:54:56.137 [kafka-admin-client-thread | adminclient-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
11:54:56.138 [kafka-admin-client-thread | adminclient-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
11:54:56.138 [kafka-admin-client-thread | adminclient-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:54:56.138 [kafka-admin-client-thread | adminclient-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
11:54:56.139 [Test worker] INFO  c.k.s.pipeline.grpc.GrpcForwarder - Shutting down gRPC channel

Failed to inject value for field [kafkaAdminClient] of class: com.krickert.search.test.platform.TestPipelineServiceProcessorTest

Message: No bean of type [org.apache.kafka.clients.admin.KafkaAdminClient] exists. 
Path Taken: 
c.k.s.t.p.TestPipelineServiceProcessorTest#kafkaAdminClient
io.micronaut.context.exceptions.DependencyInjectionException: Failed to inject value for field [kafkaAdminClient] of class: com.krickert.search.test.platform.TestPipelineServiceProcessorTest

Message: No bean of type [org.apache.kafka.clients.admin.KafkaAdminClient] exists. 
Path Taken: 
c.k.s.t.p.TestPipelineServiceProcessorTest#kafkaAdminClient
	at io.micronaut.context.AbstractInitializableBeanDefinition.resolveBean(AbstractInitializableBeanDefinition.java:2139)
	at io.micronaut.context.AbstractInitializableBeanDefinition.getBeanForField(AbstractInitializableBeanDefinition.java:1696)
	at com.krickert.search.test.platform.$TestPipelineServiceProcessorTest$Definition.inject(Unknown Source)
	at io.micronaut.context.DefaultBeanContext.doInjectAndInitialize(DefaultBeanContext.java:2663)
	at io.micronaut.context.DefaultBeanContext.inject(DefaultBeanContext.java:1022)
	at io.micronaut.test.extensions.junit5.MicronautJunit5Extension.beforeAll(MicronautJunit5Extension.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
Caused by: io.micronaut.context.exceptions.NoSuchBeanException: No bean of type [org.apache.kafka.clients.admin.KafkaAdminClient] exists. 
	at io.micronaut.context.DefaultBeanContext.newNoSuchBeanException(DefaultBeanContext.java:2800)
	at io.micronaut.context.DefaultApplicationContext.newNoSuchBeanException(DefaultApplicationContext.java:338)
	at io.micronaut.context.DefaultBeanContext.resolveBeanRegistration(DefaultBeanContext.java:2763)
	at io.micronaut.context.DefaultBeanContext.getBean(DefaultBeanContext.java:1779)
	at io.micronaut.context.AbstractBeanResolutionContext.getBean(AbstractBeanResolutionContext.java:210)
	at io.micronaut.context.AbstractInitializableBeanDefinition.resolveBean(AbstractInitializableBeanDefinition.java:2122)
	... 6 more


No bean of type [org.apache.kafka.clients.admin.KafkaAdminClient] exists. 
io.micronaut.context.exceptions.NoSuchBeanException: No bean of type [org.apache.kafka.clients.admin.KafkaAdminClient] exists. 
	at app//io.micronaut.context.DefaultBeanContext.newNoSuchBeanException(DefaultBeanContext.java:2800)
	at app//io.micronaut.context.DefaultApplicationContext.newNoSuchBeanException(DefaultApplicationContext.java:338)
	at app//io.micronaut.context.DefaultBeanContext.resolveBeanRegistration(DefaultBeanContext.java:2763)
	at app//io.micronaut.context.DefaultBeanContext.getBean(DefaultBeanContext.java:1779)
	at app//io.micronaut.context.AbstractBeanResolutionContext.getBean(AbstractBeanResolutionContext.java:210)
	at app//io.micronaut.context.AbstractInitializableBeanDefinition.resolveBean(AbstractInitializableBeanDefinition.java:2122)
	at app//io.micronaut.context.AbstractInitializableBeanDefinition.getBeanForField(AbstractInitializableBeanDefinition.java:1696)
	at app//com.krickert.search.test.platform.$TestPipelineServiceProcessorTest$Definition.inject(Unknown Source)
	at app//io.micronaut.context.DefaultBeanContext.doInjectAndInitialize(DefaultBeanContext.java:2663)
	at app//io.micronaut.context.DefaultBeanContext.inject(DefaultBeanContext.java:1022)
	at app//io.micronaut.test.extensions.junit5.MicronautJunit5Extension.beforeAll(MicronautJunit5Extension.java:90)
	at java.base@21.0.7/java.util.ArrayList.forEach(ArrayList.java:1596)


11:54:56.165 [Thread-7] INFO  c.k.s.t.p.kafka.TestContainerManager - Stopping containers managed by TestContainerManager...
> Task :pipeline-service-test-utils:pipeline-test-platform:test
TestPipelineServiceProcessorTest > initializationError FAILED
    io.micronaut.context.exceptions.DependencyInjectionException at AbstractInitializableBeanDefinition.java:2139
        Caused by: io.micronaut.context.exceptions.NoSuchBeanException at DefaultBeanContext.java:2800
11:54:56.343 [Thread-7] INFO  c.k.s.t.p.kafka.TestContainerManager - Kafka container stopped.
11:54:56.639 [Thread-7] INFO  c.k.s.t.p.kafka.TestContainerManager - Consul container stopped.
11:54:56.640 [Thread-7] INFO  c.k.s.t.p.kafka.TestContainerManager - Managed containers stopped.
> Task :pipeline-service-test-utils:pipeline-test-platform:test FAILED
1 test completed, 1 failed
[Incubating] Problems report is available at: file:///Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/build/reports/problems/problems-report.html
FAILURE: Build failed with an exception.
* What went wrong:
Execution failed for task ':pipeline-service-test-utils:pipeline-test-platform:test'.
> There were failing tests. See the report at: file:///Users/krickert/IdeaProjects/micronaut/micronaut-multiproject-example/pipeline-service-test-utils/pipeline-test-platform/build/reports/tests/test/index.html
* Try:
> Run with --scan to get full insights.
Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.
You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.
For more on this, please refer to https://docs.gradle.org/8.14/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.
BUILD FAILED in 22s
17 actionable tasks: 4 executed, 13 up-to-date


