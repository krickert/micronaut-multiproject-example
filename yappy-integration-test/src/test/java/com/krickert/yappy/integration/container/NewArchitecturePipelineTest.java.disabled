package com.krickert.yappy.integration.container;

import com.google.protobuf.ByteString;
import com.krickert.search.grpc.*;
import io.grpc.ManagedChannel;
import io.grpc.ManagedChannelBuilder;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.junit.jupiter.api.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.containers.GenericContainer;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.containers.output.Slf4jLogConsumer;
import org.testcontainers.containers.wait.strategy.Wait;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.TimeUnit;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Integration test demonstrating the new architecture where:
 * - Engine is a standalone orchestrator
 * - Modules are separate containers
 * - Modules register with the engine
 * - Pipeline processing works across containers
 */
@Testcontainers
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class NewArchitecturePipelineTest {
    
    private static final Logger log = LoggerFactory.getLogger(NewArchitecturePipelineTest.class);
    private static final Network network = Network.newNetwork();
    
    // Infrastructure containers
    @Container
    static GenericContainer<?> consulContainer = new GenericContainer<>(DockerImageName.parse("hashicorp/consul:latest"))
            .withExposedPorts(8500)
            .withNetwork(network)
            .withNetworkAliases("consul")
            .withCommand("agent", "-server", "-bootstrap-expect=1", "-client=0.0.0.0", "-ui")
            .waitingFor(Wait.forHttp("/v1/status/leader").forStatusCode(200))
            .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("CONSUL"));
    
    @Container
    static KafkaContainer kafkaContainer = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:latest"))
            .withNetwork(network)
            .withNetworkAliases("kafka");
    
    @Container
    static GenericContainer<?> apicurioContainer = new GenericContainer<>(DockerImageName.parse("apicurio/apicurio-registry-mem:latest"))
            .withExposedPorts(8080)
            .withNetwork(network)
            .withNetworkAliases("apicurio")
            .waitingFor(Wait.forHttp("/health").forStatusCode(200))
            .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("APICURIO"));
    
    // Application containers
    private static GenericContainer<?> engineContainer;
    private static GenericContainer<?> tikaParserContainer;
    private static GenericContainer<?> chunkerContainer;
    
    // gRPC clients
    private ManagedChannel registrationChannel;
    private ManagedChannel engineChannel;
    private ModuleRegistrationGrpc.ModuleRegistrationBlockingStub registrationStub;
    private PipeStreamEngineGrpc.PipeStreamEngineBlockingStub engineStub;
    
    // Kafka clients
    private KafkaProducer<String, PipeStream> kafkaProducer;
    private KafkaConsumer<String, PipeStream> kafkaConsumer;
    
    @BeforeAll
    static void setupContainers() throws Exception {
        // Start infrastructure
        consulContainer.start();
        kafkaContainer.start();
        apicurioContainer.start();
        
        // Configure pipeline in Consul
        configurePipelineInConsul();
        
        // Start Engine
        engineContainer = new GenericContainer<>(DockerImageName.parse("yappy-engine:latest"))
                .withExposedPorts(8080, 50050, 50051)
                .withNetwork(network)
                .withNetworkAliases("engine")
                .withEnv("CONSUL_HOST", "consul")
                .withEnv("CONSUL_PORT", "8500")
                .withEnv("KAFKA_BOOTSTRAP_SERVERS", kafkaContainer.getNetworkAliases().get(0) + ":9092")
                .withEnv("APICURIO_REGISTRY_URL", "http://apicurio:8080")
                .withEnv("YAPPY_ENGINE_NAME", "test-engine")
                .withEnv("YAPPY_CLUSTER_NAME", "test-cluster")
                .waitingFor(Wait.forHttp("/health").forPort(8080).forStatusCode(200))
                .withStartupTimeout(Duration.ofMinutes(2))
                .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("ENGINE"));
        
        engineContainer.start();
        
        // Start Modules
        tikaParserContainer = new GenericContainer<>(DockerImageName.parse("yappy-tika-parser:latest"))
                .withExposedPorts(50053)
                .withNetwork(network)
                .withNetworkAliases("tika-parser")
                .waitingFor(Wait.forLogMessage(".*gRPC server started.*", 1))
                .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("TIKA"));
        
        chunkerContainer = new GenericContainer<>(DockerImageName.parse("yappy-chunker:latest"))
                .withExposedPorts(50052)
                .withNetwork(network)
                .withNetworkAliases("chunker")
                .waitingFor(Wait.forLogMessage(".*gRPC server started.*", 1))
                .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("CHUNKER"));
        
        tikaParserContainer.start();
        chunkerContainer.start();
        
        // Wait a bit for everything to stabilize
        Thread.sleep(5000);
    }
    
    @BeforeEach
    void setup() {
        // Setup gRPC channels
        registrationChannel = ManagedChannelBuilder
                .forAddress(engineContainer.getHost(), engineContainer.getMappedPort(50051))
                .usePlaintext()
                .build();
        
        engineChannel = ManagedChannelBuilder
                .forAddress(engineContainer.getHost(), engineContainer.getMappedPort(50050))
                .usePlaintext()
                .build();
        
        registrationStub = ModuleRegistrationGrpc.newBlockingStub(registrationChannel);
        engineStub = PipeStreamEngineGrpc.newBlockingStub(engineChannel);
        
        // Setup Kafka
        setupKafkaClients();
    }
    
    @AfterEach
    void cleanup() {
        if (registrationChannel != null) registrationChannel.shutdown();
        if (engineChannel != null) engineChannel.shutdown();
        if (kafkaProducer != null) kafkaProducer.close();
        if (kafkaConsumer != null) kafkaConsumer.close();
    }
    
    @Test
    @Order(1)
    void testRegisterModules() {
        // Register Tika Parser
        ModuleInfo tikaInfo = ModuleInfo.newBuilder()
                .setServiceName("tika-parser")
                .setServiceId("tika-parser-001")
                .setHost("tika-parser")
                .setPort(50053)
                .setHealthEndpoint("/grpc.health.v1.Health/Check")
                .putMetadata("version", "1.0.0")
                .build();
        
        RegistrationStatus tikaStatus = registrationStub.registerModule(tikaInfo);
        assertTrue(tikaStatus.getSuccess(), "Tika registration should succeed");
        
        // Register Chunker
        ModuleInfo chunkerInfo = ModuleInfo.newBuilder()
                .setServiceName("chunker")
                .setServiceId("chunker-001")
                .setHost("chunker")
                .setPort(50052)
                .setHealthEndpoint("/grpc.health.v1.Health/Check")
                .putMetadata("version", "1.0.0")
                .build();
        
        RegistrationStatus chunkerStatus = registrationStub.registerModule(chunkerInfo);
        assertTrue(chunkerStatus.getSuccess(), "Chunker registration should succeed");
        
        // Verify both are registered
        ModuleList modules = registrationStub.listModules(com.google.protobuf.Empty.getDefaultInstance());
        assertEquals(2, modules.getModulesCount(), "Should have 2 modules registered");
    }
    
    @Test
    @Order(2)
    void testGrpcPipelineProcessing() {
        // Create a test document
        PipeDoc document = PipeDoc.newBuilder()
                .setId("test-doc-001")
                .setSourceName("test-source")
                .putMetadata("content_type", "text/plain")
                .addPrimaryPayload(PipeDocPayload.newBuilder()
                        .setFormat("text/plain")
                        .setRawData(ByteString.copyFromUtf8("This is a test document for processing through the pipeline."))
                        .build())
                .build();
        
        // Create PipeStream
        PipeStream pipeStream = PipeStream.newBuilder()
                .setStreamId("stream-" + UUID.randomUUID())
                .setCurrentPipelineName("test-pipeline")
                .setTargetStepName("parse-document")
                .setDocument(document)
                .build();
        
        // Send to engine
        ProcessResponse response = engineStub.processPipe(pipeStream);
        
        // Verify response
        assertTrue(response.getSuccess(), "Processing should succeed");
        assertNotNull(response.getOutputDoc(), "Should have output document");
        assertEquals("test-doc-001", response.getOutputDoc().getId(), "Document ID should match");
        
        // Check that document was processed (should have extracted text)
        assertTrue(response.getOutputDoc().hasExtractedText(), "Should have extracted text");
        assertFalse(response.getOutputDoc().getExtractedText().isEmpty(), "Extracted text should not be empty");
        
        log.info("Document processed successfully with extracted text: {}", 
                response.getOutputDoc().getExtractedText());
    }
    
    @Test
    @Order(3)
    void testKafkaPipelineProcessing() throws Exception {
        // Create a test document
        PipeDoc document = PipeDoc.newBuilder()
                .setId("kafka-doc-001")
                .setSourceName("kafka-source")
                .putMetadata("content_type", "text/plain")
                .addPrimaryPayload(PipeDocPayload.newBuilder()
                        .setFormat("text/plain")
                        .setRawData(ByteString.copyFromUtf8("This is a Kafka test document that needs chunking."))
                        .build())
                .build();
        
        // Create PipeStream for Kafka
        PipeStream pipeStream = PipeStream.newBuilder()
                .setStreamId("kafka-stream-" + UUID.randomUUID())
                .setCurrentPipelineName("test-pipeline")
                .setTargetStepName("chunk-text")
                .setDocument(document)
                .build();
        
        // Send to Kafka input topic
        ProducerRecord<String, PipeStream> record = new ProducerRecord<>(
                "test-pipeline-input", 
                pipeStream.getStreamId(), 
                pipeStream
        );
        kafkaProducer.send(record).get();
        
        // Subscribe to output topic
        kafkaConsumer.subscribe(Collections.singletonList("test-pipeline-output"));
        
        // Poll for results
        ConsumerRecords<String, PipeStream> records = kafkaConsumer.poll(Duration.ofSeconds(30));
        
        assertFalse(records.isEmpty(), "Should receive processed messages");
        
        // Verify the processed document
        PipeStream processedStream = records.iterator().next().value();
        assertNotNull(processedStream);
        assertEquals("kafka-doc-001", processedStream.getDocument().getId());
        
        // Should have semantic results from chunking
        assertTrue(processedStream.getDocument().getSemanticResultsCount() > 0, 
                "Should have chunked results");
        
        log.info("Document processed via Kafka with {} chunks", 
                processedStream.getDocument().getSemanticResultsCount());
    }
    
    @Test
    @Order(4)
    void testFullPipelineWithMultipleSteps() {
        // Create a document that needs both parsing and chunking
        PipeDoc document = PipeDoc.newBuilder()
                .setId("full-pipeline-001")
                .setSourceName("full-test")
                .putMetadata("content_type", "application/pdf")
                .addPrimaryPayload(PipeDocPayload.newBuilder()
                        .setFormat("application/pdf")
                        .setRawData(ByteString.copyFromUtf8("Mock PDF content that needs extraction and chunking"))
                        .build())
                .build();
        
        // Start with parse step
        PipeStream pipeStream = PipeStream.newBuilder()
                .setStreamId("full-stream-" + UUID.randomUUID())
                .setCurrentPipelineName("test-pipeline")
                .setTargetStepName("parse-document")
                .setDocument(document)
                .build();
        
        // Process through engine
        ProcessResponse parseResponse = engineStub.processPipe(pipeStream);
        assertTrue(parseResponse.getSuccess(), "Parsing should succeed");
        
        // Now send the parsed document for chunking
        PipeStream chunkStream = PipeStream.newBuilder()
                .setStreamId(pipeStream.getStreamId())
                .setCurrentPipelineName("test-pipeline")
                .setTargetStepName("chunk-text")
                .setDocument(parseResponse.getOutputDoc())
                .setCurrentHopNumber(1)
                .build();
        
        ProcessResponse chunkResponse = engineStub.processPipe(chunkStream);
        assertTrue(chunkResponse.getSuccess(), "Chunking should succeed");
        
        // Verify full processing
        PipeDoc finalDoc = chunkResponse.getOutputDoc();
        assertTrue(finalDoc.hasExtractedText(), "Should have extracted text");
        assertTrue(finalDoc.getSemanticResultsCount() > 0, "Should have chunks");
        
        log.info("Full pipeline processed: extracted text and created {} chunks", 
                finalDoc.getSemanticResultsCount());
    }
    
    private static void configurePipelineInConsul() throws Exception {
        // This would normally configure the pipeline in Consul
        // For now, we'll assume the engine has a default configuration
        log.info("Pipeline configuration would be set up in Consul here");
    }
    
    private void setupKafkaClients() {
        // Producer config
        Properties producerProps = new Properties();
        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaContainer.getBootstrapServers());
        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                "org.apache.kafka.common.serialization.StringSerializer");
        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                "io.apicurio.registry.serde.protobuf.ProtobufKafkaSerializer");
        producerProps.put("apicurio.registry.url", 
                "http://" + apicurioContainer.getHost() + ":" + apicurioContainer.getMappedPort(8080));
        
        kafkaProducer = new KafkaProducer<>(producerProps);
        
        // Consumer config
        Properties consumerProps = new Properties();
        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaContainer.getBootstrapServers());
        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, "test-consumer-group");
        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                "org.apache.kafka.common.serialization.StringDeserializer");
        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                "io.apicurio.registry.serde.protobuf.ProtobufKafkaDeserializer");
        consumerProps.put("apicurio.registry.url", 
                "http://" + apicurioContainer.getHost() + ":" + apicurioContainer.getMappedPort(8080));
        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        kafkaConsumer = new KafkaConsumer<>(consumerProps);
    }
}